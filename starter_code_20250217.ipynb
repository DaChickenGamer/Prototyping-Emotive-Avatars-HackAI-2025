{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Tb_r8SYY6_"
      },
      "source": [
        "## Starter Code\n",
        "\n",
        "<span style=\"color:blue\">**In this guide, we provide a starter code to help you begin your project.\n",
        "Please feel free to use it thoughtfully and tailor it to your specific requirements.**</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLTTDzCkYk2J",
        "outputId": "a601a326-0f9a-4da7-d2d0-578c19f13209"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaYmec1YY7A"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "**Facial Recognition**\n",
        "\n",
        "Dataset: FER2013\n",
        "- CSV Format: Kaggle Link https://www.kaggle.com/datasets/nicolejyt/facialexpressionrecognition\n",
        "- JPG Format: Kaggle Link https://www.kaggle.com/datasets/msambare/fer2013/data\n",
        "- Details:\n",
        "    - 34,034 images\n",
        "    - 48x48 pixels\n",
        "    - 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
        "\n",
        "**Gesture Recognition**\n",
        "\n",
        "Dataset: LeapGestureRecognition\n",
        "\n",
        "- Download Link: Kaggle https://www.kaggle.com/datasets/gti-upm/leapgestrecog\n",
        "- Other Gesture Dataset: GitHub https://github.com/linto-ai/multi-hand-gesture-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsydy3zlYY7A"
      },
      "source": [
        "### Examples & Inspirations:\n",
        "\n",
        "#### Facial Expression\n",
        "\n",
        "1. Static and dynamic facial emotion recognition using the Emo-AffectNet\n",
        "\n",
        "   https://huggingface.co/ElenaRyumina/face_emotion_recognition\n",
        "\n",
        "\n",
        "   ![example](https://github.com/ElenaRyumina/EMO-AffectNetModel/blob/main/gif/result_2.gif?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbCHconRYY7A"
      },
      "source": [
        "2. Facial Expression Recognition Challenge\n",
        "\n",
        "   https://github.com/chinhau-lim/fer_2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiXA61T5YY7B"
      },
      "source": [
        "#### Hand Gesture\n",
        "\n",
        "1. Real-time hand gesture recognition using TensorFlow & OpenCV\n",
        "\n",
        "   https://techvidvan.com/tutorials/hand-gesture-recognition-tensorflow-opencv/\n",
        "\n",
        "    ![example](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2021/07/landmark-output.gif)\n",
        "\n",
        "\n",
        "\n",
        "2. Deep_learning_hand_gesture_recognition\n",
        "\n",
        "   https://github.com/guillaumephd/deep_learning_hand_gesture_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qhgi7A3dYY7B"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip3 install opencv-python numpy pandas tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "q9RXfgS9YY7C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9qRxgAYY7C"
      },
      "source": [
        "### 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3g7p6qj8YY7C"
      },
      "outputs": [],
      "source": [
        "# Facial Emotion Classes\n",
        "EMOTION_MAP = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Gesture Classes (LeapGestRecog)\n",
        "GESTURE_MAP = {\n",
        "    0: 'palm',        # 01_palm\n",
        "    1: 'l',           # 02_l\n",
        "    2: 'fist',        # 03_fist\n",
        "    3: 'fist_moved',  # 04_fist_moved\n",
        "    4: 'thumb',       # 05_thumb\n",
        "    5: 'index',       # 06_index\n",
        "    6: 'ok',          # 07_ok\n",
        "    7: 'palm_moved',  # 08_palm_moved\n",
        "    8: 'c',           # 09_c\n",
        "    9: 'down'         # 10_down\n",
        "}\n",
        "\n",
        "def load_fer2013(file_path='/content/drive/MyDrive/Prototype Emotive Avatar Data/fer2013.csv'):\n",
        "    \"\"\"Loads and balances FER2013 dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    pixels = df['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
        "    images = np.array([x.reshape(48, 48, 1) for x in pixels]) / 255.0\n",
        "    labels = df['emotion'].values\n",
        "\n",
        "    # Handle class imbalance\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    return images, labels, dict(enumerate(class_weights))\n",
        "\n",
        "def load_leap_gestures(dataset_path=\"/content/drive/MyDrive/Prototype Emotive Avatar Data/leapGestRecog\"):\n",
        "    \"\"\"Loads Leap Motion dataset with correct label mapping\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        try:\n",
        "            folder_name = os.path.basename(root)\n",
        "            if not folder_name[:2].isdigit():\n",
        "                continue\n",
        "\n",
        "            gesture_num = int(folder_name[:2]) - 1\n",
        "            if gesture_num not in GESTURE_MAP:\n",
        "                continue\n",
        "\n",
        "            # Process images\n",
        "            for file in files:\n",
        "                if file.endswith(\".png\"):\n",
        "                    img = cv2.imread(os.path.join(root, file), cv2.IMREAD_GRAYSCALE)\n",
        "                    img = cv2.resize(img, (64, 64))\n",
        "                    X.append(img.astype('float32') / 255.0)\n",
        "                    y.append(gesture_num)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {root}: {str(e)}\")\n",
        "\n",
        "    return np.array(X).reshape(-1, 64, 64, 1), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP_hW1SUYY7C"
      },
      "source": [
        "### 2. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8JivrIYAYY7C"
      },
      "outputs": [],
      "source": [
        "class EmotiveAvatarSystem:\n",
        "    def __init__(self):\n",
        "        self.face_model = self.build_face_model()\n",
        "        self.gesture_model = self.build_gesture_model()\n",
        "\n",
        "    def build_face_model(self):\n",
        "        resize_and_rescale = models.Sequential([\n",
        "            layers.Resizing(48, 48),\n",
        "            layers.Rescaling(1./255)\n",
        "        ])\n",
        "\n",
        "        data_augmentation = models.Sequential([\n",
        "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            layers.RandomRotation(0.2),\n",
        "            layers.RandomBrightness(.2),\n",
        "            layers.RandomContrast(.2),\n",
        "            layers.RandomCrop(.2),\n",
        "            layers.RandomZoom(.2)\n",
        "        ])\n",
        "\n",
        "        model = models.Sequential([\n",
        "            resize_and_rescale,\n",
        "            data_augmentation,\n",
        "            layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
        "            layers.MaxPool2D((2, 2)),\n",
        "\n",
        "            layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Conv2D(64, (3,3), activation='relu'),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.4),\n",
        "\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(7, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_gesture_model(self):\n",
        "        model = models.Sequential([\n",
        "            layers.Conv2D(16, (3,3), activation='relu', input_shape=(64,64,1)),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            layers.Conv2D(32, (3,3), activation='relu'),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.35),\n",
        "\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyq_Ti6QYY7C"
      },
      "source": [
        "### 3. Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XckoqkmYY7C",
        "outputId": "71975ce3-8acd-42fe-9aef-b06d2c72060d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Facial Model (FER2013)...\n",
            "Epoch 1/5\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 125ms/step - accuracy: 0.1571 - loss: 1.9365 - val_accuracy: 0.1719 - val_loss: 1.9156 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 119ms/step - accuracy: 0.1782 - loss: 1.8917 - val_accuracy: 0.1308 - val_loss: 1.9245 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 120ms/step - accuracy: 0.1568 - loss: 1.9380 - val_accuracy: 0.1912 - val_loss: 1.9176 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 125ms/step - accuracy: 0.1846 - loss: 1.9154 - val_accuracy: 0.2165 - val_loss: 1.8984 - learning_rate: 5.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 132ms/step - accuracy: 0.1975 - loss: 1.9037 - val_accuracy: 0.1907 - val_loss: 1.9208 - learning_rate: 5.0000e-04\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.2277 - loss: 1.9020\n",
            "\n",
            "=== Final Performance ===\n",
            "Facial Accuracy: 22.07%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    X_face, y_face, face_weights = load_fer2013()\n",
        "    #X_gesture, y_gesture = load_leap_gestures()\n",
        "\n",
        "    # Split datasets\n",
        "    Xf_train, Xf_test, yf_train, yf_test = train_test_split(X_face, y_face, test_size=0.2, stratify=y_face)\n",
        "    #Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_gesture, y_gesture, test_size=0.2, stratify=y_gesture)\n",
        "\n",
        "    # Initialize system\n",
        "    avatar = EmotiveAvatarSystem()\n",
        "\n",
        "    # Train facial model\n",
        "    print(\"Training Facial Model (FER2013)...\")\n",
        "    avatar.face_model.fit(Xf_train, yf_train,\n",
        "                         epochs=5,\n",
        "                         batch_size=64,\n",
        "                         class_weight=face_weights,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[\n",
        "                             callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                             callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
        "                         ])\n",
        "\n",
        "    # Train gesture model\n",
        "    #print(\"\\nTraining Gesture Model (Leap Motion)...\")\n",
        "    # avatar.gesture_model.fit(Xg_train, yg_train,\n",
        "    #                         epochs=0,\n",
        "    #                         batch_size=32,\n",
        "    #                         validation_split=0.2,\n",
        "    #                         callbacks=[\n",
        "    #                             callbacks.EarlyStopping(patience=3)\n",
        "    #                         ])\n",
        "\n",
        "    # Evaluate\n",
        "    f_loss, f_acc = avatar.face_model.evaluate(Xf_test, yf_test)\n",
        "   # g_loss, g_acc = avatar.gesture_model.evaluate(Xg_test, yg_test)\n",
        "\n",
        "    print(\"\\n=== Final Performance ===\")\n",
        "    print(f\"Facial Accuracy: {f_acc:.2%}\")\n",
        "    #print(f\"Gesture Accuracy: {g_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejfI03I_J4vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZnUk8RYY7D"
      },
      "source": [
        "### Improvement Opportunities:\n",
        "1. Add real-time webcam integration\n",
        "2. Implement data augmentation for infrared images\n",
        "4. Add attention mechanisms to the CNN\n",
        "5. ......"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zkCbQhYY7D"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://github.com/chinhau-lim/fer_2013\n",
        "\n",
        "https://arxiv.org/pdf/2105.03588\n",
        "\n",
        "https://github.com/takanto/FER2013\n",
        "\n",
        "https://aicompetence.org/ai-in-cultural-sensitivity-gesture-interpretation/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Display"
      ],
      "metadata": {
        "id": "9F_ry2gba9gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Website Requirements"
      ],
      "metadata": {
        "id": "qYOaecQVl0v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egybNDSCl0TD",
        "outputId": "e4f9d52c-9b86-41c4-f24d-11bb05a94695"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `ngrok authtoken userdata.get('ngrok-api-key')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Website Display"
      ],
      "metadata": {
        "id": "Xcbh3aujm1eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Retrieve the ngrok API key from Google Secrets\n",
        "ngrok_api_key = userdata.get('ngrok-api-key')\n",
        "\n",
        "# Check if the API key was retrieved successfully\n",
        "print(f\"API Key retrieved: {ngrok_api_key}\")\n",
        "\n",
        "# Authenticate ngrok with the API key\n",
        "ngrok.set_auth_token(ngrok_api_key)\n",
        "\n",
        "# Set up a simple Flask app\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello from Flask with ngrok on Google Colab!\"\n",
        "\n",
        "# Open a ngrok tunnel to the Flask app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> http://127.0.0.1:5000\")\n",
        "\n",
        "# Run the Flask app\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7gxDsFWm3lw",
        "outputId": "957d9c17-1f3b-4037-d9a8-1be6c8e9a438"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key retrieved: 2tPBu2msmL9TTO6Blg5WZJrZPTR_71eHnJbyBRb7vKaAK2s2v\n",
            " * ngrok tunnel \"NgrokTunnel: \"https://4e2c-34-125-101-168.ngrok-free.app\" -> \"http://localhost:5000\"\" -> http://127.0.0.1:5000\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:15:32] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:15:32] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:16:40] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:16:40] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}