{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Tb_r8SYY6_"
      },
      "source": [
        "## Starter Code\n",
        "\n",
        "<span style=\"color:blue\">**In this guide, we provide a starter code to help you begin your project.\n",
        "Please feel free to use it thoughtfully and tailor it to your specific requirements.**</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BLTTDzCkYk2J",
        "outputId": "a601a326-0f9a-4da7-d2d0-578c19f13209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaYmec1YY7A"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "**Facial Recognition**\n",
        "\n",
        "Dataset: FER2013\n",
        "- CSV Format: Kaggle Link https://www.kaggle.com/datasets/nicolejyt/facialexpressionrecognition\n",
        "- JPG Format: Kaggle Link https://www.kaggle.com/datasets/msambare/fer2013/data\n",
        "- Details:\n",
        "    - 34,034 images\n",
        "    - 48x48 pixels\n",
        "    - 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
        "\n",
        "**Gesture Recognition**\n",
        "\n",
        "Dataset: LeapGestureRecognition\n",
        "\n",
        "- Download Link: Kaggle https://www.kaggle.com/datasets/gti-upm/leapgestrecog\n",
        "- Other Gesture Dataset: GitHub https://github.com/linto-ai/multi-hand-gesture-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsydy3zlYY7A"
      },
      "source": [
        "### Examples & Inspirations:\n",
        "\n",
        "#### Facial Expression\n",
        "\n",
        "1. Static and dynamic facial emotion recognition using the Emo-AffectNet\n",
        "\n",
        "   https://huggingface.co/ElenaRyumina/face_emotion_recognition\n",
        "\n",
        "\n",
        "   ![example](https://github.com/ElenaRyumina/EMO-AffectNetModel/blob/main/gif/result_2.gif?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbCHconRYY7A"
      },
      "source": [
        "2. Facial Expression Recognition Challenge\n",
        "\n",
        "   https://github.com/chinhau-lim/fer_2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiXA61T5YY7B"
      },
      "source": [
        "#### Hand Gesture\n",
        "\n",
        "1. Real-time hand gesture recognition using TensorFlow & OpenCV\n",
        "\n",
        "   https://techvidvan.com/tutorials/hand-gesture-recognition-tensorflow-opencv/\n",
        "\n",
        "    ![example](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2021/07/landmark-output.gif)\n",
        "\n",
        "\n",
        "\n",
        "2. Deep_learning_hand_gesture_recognition\n",
        "\n",
        "   https://github.com/guillaumephd/deep_learning_hand_gesture_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qhgi7A3dYY7B"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip3 install opencv-python numpy pandas tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q9RXfgS9YY7C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9qRxgAYY7C"
      },
      "source": [
        "### 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3g7p6qj8YY7C"
      },
      "outputs": [],
      "source": [
        "# Facial Emotion Classes\n",
        "EMOTION_MAP = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Gesture Classes (LeapGestRecog)\n",
        "GESTURE_MAP = {\n",
        "    0: 'palm',        # 01_palm\n",
        "    1: 'l',           # 02_l\n",
        "    2: 'fist',        # 03_fist\n",
        "    3: 'fist_moved',  # 04_fist_moved\n",
        "    4: 'thumb',       # 05_thumb\n",
        "    5: 'index',       # 06_index\n",
        "    6: 'ok',          # 07_ok\n",
        "    7: 'palm_moved',  # 08_palm_moved\n",
        "    8: 'c',           # 09_c\n",
        "    9: 'down'         # 10_down\n",
        "}\n",
        "\n",
        "def load_fer2013(file_path='/content/drive/MyDrive/Prototype Emotive Avatar Data/fer2013.csv'):\n",
        "    \"\"\"Loads and balances FER2013 dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    pixels = df['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
        "    images = np.array([x.reshape(48, 48, 1) for x in pixels]) / 255.0\n",
        "    labels = df['emotion'].values\n",
        "\n",
        "    # Handle class imbalance\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    return images, labels, dict(enumerate(class_weights))\n",
        "\n",
        "def load_leap_gestures(dataset_path=\"/content/drive/MyDrive/Prototype Emotive Avatar Data/leapGestRecog\"):\n",
        "    \"\"\"Loads Leap Motion dataset with correct label mapping\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        try:\n",
        "            folder_name = os.path.basename(root)\n",
        "            if not folder_name[:2].isdigit():\n",
        "                continue\n",
        "\n",
        "            gesture_num = int(folder_name[:2]) - 1\n",
        "            if gesture_num not in GESTURE_MAP:\n",
        "                continue\n",
        "\n",
        "            # Process images\n",
        "            for file in files:\n",
        "                if file.endswith(\".png\"):\n",
        "                    img = cv2.imread(os.path.join(root, file), cv2.IMREAD_GRAYSCALE)\n",
        "                    img = cv2.resize(img, (64, 64))\n",
        "                    X.append(img.astype('float32') / 255.0)\n",
        "                    y.append(gesture_num)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {root}: {str(e)}\")\n",
        "\n",
        "    return np.array(X).reshape(-1, 64, 64, 1), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP_hW1SUYY7C"
      },
      "source": [
        "### 2. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8JivrIYAYY7C"
      },
      "outputs": [],
      "source": [
        "class EmotiveAvatarSystem:\n",
        "    def __init__(self):\n",
        "        self.face_model = self.build_face_model()\n",
        "        self.gesture_model = self.build_gesture_model()\n",
        "\n",
        "    def build_face_model(self):\n",
        "        model = models.Sequential([\n",
        "            layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Conv2D(64, (3,3), activation='relu'),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.4),\n",
        "\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(7, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_gesture_model(self):\n",
        "        model = models.Sequential([\n",
        "            layers.Conv2D(16, (3,3), activation='relu', input_shape=(64,64,1)),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            layers.Conv2D(32, (3,3), activation='relu'),\n",
        "            layers.MaxPooling2D((2,2)),\n",
        "            layers.Dropout(0.35),\n",
        "\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyq_Ti6QYY7C"
      },
      "source": [
        "### 3. Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7XckoqkmYY7C",
        "outputId": "7c0a1934-820b-4ffa-ffb7-b9a97f4492b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Facial Model (FER2013)...\n",
            "Epoch 1/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 173ms/step - accuracy: 0.1395 - loss: 1.9337 - val_accuracy: 0.1738 - val_loss: 1.9233 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 170ms/step - accuracy: 0.1831 - loss: 1.8930 - val_accuracy: 0.1703 - val_loss: 1.9186 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 174ms/step - accuracy: 0.1757 - loss: 1.9152 - val_accuracy: 0.1928 - val_loss: 1.8981 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 169ms/step - accuracy: 0.1794 - loss: 1.9160 - val_accuracy: 0.1620 - val_loss: 1.9199 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 168ms/step - accuracy: 0.1909 - loss: 1.8718 - val_accuracy: 0.1426 - val_loss: 1.9177 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 168ms/step - accuracy: 0.1796 - loss: 1.8866 - val_accuracy: 0.1853 - val_loss: 1.8849 - learning_rate: 5.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 175ms/step - accuracy: 0.1859 - loss: 1.8956 - val_accuracy: 0.1829 - val_loss: 1.8930 - learning_rate: 5.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 168ms/step - accuracy: 0.1863 - loss: 1.8776 - val_accuracy: 0.2024 - val_loss: 1.8673 - learning_rate: 5.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 175ms/step - accuracy: 0.1846 - loss: 1.8723 - val_accuracy: 0.1635 - val_loss: 1.9049 - learning_rate: 5.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 175ms/step - accuracy: 0.1900 - loss: 1.8694 - val_accuracy: 0.1726 - val_loss: 1.8760 - learning_rate: 5.0000e-04\n",
            "\n",
            "Training Gesture Model (Leap Motion)...\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.2082 - loss: 1.8591\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 2.3037\n",
            "\n",
            "=== Final Performance ===\n",
            "Facial Accuracy: 20.62%\n",
            "Gesture Accuracy: 0.00%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    X_face, y_face, face_weights = load_fer2013()\n",
        "    X_gesture, y_gesture = load_leap_gestures()\n",
        "\n",
        "    # Split datasets\n",
        "    Xf_train, Xf_test, yf_train, yf_test = train_test_split(X_face, y_face, test_size=0.2, stratify=y_face)\n",
        "    Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_gesture, y_gesture, test_size=0.2, stratify=y_gesture)\n",
        "\n",
        "    # Initialize system\n",
        "    avatar = EmotiveAvatarSystem()\n",
        "\n",
        "    # Train facial model\n",
        "    print(\"Training Facial Model (FER2013)...\")\n",
        "    avatar.face_model.fit(Xf_train, yf_train,\n",
        "                         epochs=5,\n",
        "                         batch_size=64,\n",
        "                         class_weight=face_weights,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[\n",
        "                             callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                             callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
        "                         ])\n",
        "\n",
        "    # Train gesture model\n",
        "    print(\"\\nTraining Gesture Model (Leap Motion)...\")\n",
        "    avatar.gesture_model.fit(Xg_train, yg_train,\n",
        "                            epochs=0,\n",
        "                            batch_size=32,\n",
        "                            validation_split=0.2,\n",
        "                            callbacks=[\n",
        "                                callbacks.EarlyStopping(patience=3)\n",
        "                            ])\n",
        "\n",
        "    # Evaluate\n",
        "    f_loss, f_acc = avatar.face_model.evaluate(Xf_test, yf_test)\n",
        "    g_loss, g_acc = avatar.gesture_model.evaluate(Xg_test, yg_test)\n",
        "\n",
        "    print(\"\\n=== Final Performance ===\")\n",
        "    print(f\"Facial Accuracy: {f_acc:.2%}\")\n",
        "    print(f\"Gesture Accuracy: {g_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZnUk8RYY7D"
      },
      "source": [
        "### Improvement Opportunities:\n",
        "1. Add real-time webcam integration\n",
        "2. Implement data augmentation for infrared images\n",
        "4. Add attention mechanisms to the CNN\n",
        "5. ......"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zkCbQhYY7D"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://github.com/chinhau-lim/fer_2013\n",
        "\n",
        "https://arxiv.org/pdf/2105.03588\n",
        "\n",
        "https://github.com/takanto/FER2013\n",
        "\n",
        "https://aicompetence.org/ai-in-cultural-sensitivity-gesture-interpretation/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Display"
      ],
      "metadata": {
        "id": "9F_ry2gba9gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Website Requirements"
      ],
      "metadata": {
        "id": "qYOaecQVl0v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyngrok"
      ],
      "metadata": {
        "id": "egybNDSCl0TD",
        "outputId": "e4f9d52c-9b86-41c4-f24d-11bb05a94695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `ngrok authtoken userdata.get('ngrok-api-key')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Website Display"
      ],
      "metadata": {
        "id": "Xcbh3aujm1eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Retrieve the ngrok API key from Google Secrets\n",
        "ngrok_api_key = userdata.get('ngrok-api-key')\n",
        "\n",
        "# Check if the API key was retrieved successfully\n",
        "print(f\"API Key retrieved: {ngrok_api_key}\")\n",
        "\n",
        "# Authenticate ngrok with the API key\n",
        "ngrok.set_auth_token(ngrok_api_key)\n",
        "\n",
        "# Set up a simple Flask app\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello from Flask with ngrok on Google Colab!\"\n",
        "\n",
        "# Open a ngrok tunnel to the Flask app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> http://127.0.0.1:5000\")\n",
        "\n",
        "# Run the Flask app\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "x7gxDsFWm3lw",
        "outputId": "957d9c17-1f3b-4037-d9a8-1be6c8e9a438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key retrieved: 2tPBu2msmL9TTO6Blg5WZJrZPTR_71eHnJbyBRb7vKaAK2s2v\n",
            " * ngrok tunnel \"NgrokTunnel: \"https://4e2c-34-125-101-168.ngrok-free.app\" -> \"http://localhost:5000\"\" -> http://127.0.0.1:5000\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:15:32] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:15:32] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:16:40] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Feb/2025 17:16:40] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}